{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMenCbQ0l/e8RS9epSS6EEB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jchanlau/NLP_colab/blob/main/nlp_pilot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Pilot program to distill text from PDF and perform sentiment analysis**"
      ],
      "metadata": {
        "id": "tV8ZAw3TYntR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NEzXRyephSGZ"
      },
      "outputs": [],
      "source": [
        "!pip install transformers        # Hugging Face transfomers\n",
        "!pip install --upgrade pymupdf   # util to manipulate PDF files\n",
        "!pip install nltk\n",
        "# !pip install PyPDF2            # alternative \n",
        "\n",
        "from transformers import pipeline # import Hugging Face\n",
        "import fitz # pymupdf\n",
        "import nltk # parse sentences in paragraph\n",
        "nltk.download('punkt')\n",
        "import numpy as np; import pandas as pd  # standard libraries\n",
        "import os   # in Google Colab, the home directory is content, \n",
        "            # need to store data files in content/data\n",
        "import re\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Useful functions\n",
        "\n",
        "# find text between first and last value, avoids repetitions\n",
        "def find_between_r( s, first, last ):\n",
        "    try:\n",
        "        start = s.rindex( first ) + len( first )\n",
        "        end = s.rindex( last, start )\n",
        "        return s[start:end]\n",
        "    except ValueError:\n",
        "        return \"\"\n",
        "\n",
        "# find text between first and last value\n",
        "def find_between( s, first, last ):\n",
        "    try:\n",
        "        start = s.index( first ) + len( first )\n",
        "        end = s.index( last, start )\n",
        "        return s[start:end]\n",
        "    except ValueError:\n",
        "        return \"\"\n",
        "\n",
        "# Parse section into sentences\n",
        "def parse_sentences(text, classifier, min_chars = 50):\n",
        "  sent = nltk.sent_tokenize(text)\n",
        "  sentences = []\n",
        "  sentiment = []\n",
        "  for s in sent:\n",
        "    if len(s)> min_chars:\n",
        "      sentiment.append(pd.DataFrame(classifier(s)))\n",
        "      sentences.append(s) \n",
        "  sentiment = pd.concat(sentiment).reset_index()\n",
        "  return sentences, sentiment\n",
        "\n",
        "# check if sentences match sentiment\n",
        "def examine_sentiment(sentiment_list, sentence_list, sentiment):\n",
        "  idx = sentiment_list.index[sentiment_list.label==sentiment].tolist()\n",
        "  return [sentence_list[i] for i in idx]\n",
        "\n",
        "# Parse section in paragraphs\n",
        "def parse_paragraph(text, classifier, min_chars = 20):\n",
        "  t = text.replace('\\n', ' ')\n",
        "  t = re.split(r'\\s\\d{1,2}\\.\\s', t)\n",
        "  paragraphs = []\n",
        "  sentiment = []\n",
        "  for s in t:\n",
        "    if len(s) > min_chars:\n",
        "      sentiment.append(pd.DataFrame(classifier(s)))\n",
        "      paragraphs.append(s)   \n",
        "  sentiment = pd.concat(sentiment).reset_index()\n",
        "  return paragraphs, sentiment\n"
      ],
      "metadata": {
        "id": "KZJszg1sYzxD"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get text\n",
        "doc = fitz.open(\"data/1PEREA2021001.pdf\")\n",
        "text= \"\"\n",
        "for page in doc:\n",
        "    text += page.get_text()\n",
        "doc.close()\n",
        "\n",
        "context = find_between_r(text, \"CONTEXT\", \"RECENT DEVELOPMENTS\")\n",
        "#print(context)\n",
        "#context_sentences = nltk.sent_tokenize(context_section)\n",
        "#context_sentences = parse_sentences(context)\n"
      ],
      "metadata": {
        "id": "BYD-j9bZcwUD"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classifier = pipeline(\"sentiment-analysis\")\n",
        "\n",
        "sentence_list, sentence_sa = parse_sentences(context, classifier)\n",
        "paragraph_list, paragraph_sa= parse_paragraph(context, classifier)\n",
        "print(sentence_sa)\n",
        "\n",
        "examine_sentiment(sentence_sa, sentence_list, 'NEGATIVE')\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "MzJYZy0NcAln"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}